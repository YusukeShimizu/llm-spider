---
title: Use as a Rust library
description: Call llm-spider from Rust (crawl + Markdown output).
---

`llm-spider` is both a CLI and a Rust library.
The library API lets you run a constrained crawl and produce Markdown that includes sources and `TrustTier`.

## Add as a dependency

This repository is not published to crates.io by default.
Use a path (or git) dependency.

```toml
[dependencies]
anyhow = "1"
llm-spider = { path = "../llm-spider" }
```

## Required environment variables

- `OPENAI_API_KEY`: OpenAI API key (required)
- `OPENAI_BASE_URL`: Override the API base URL (optional)
- `LLM_SPIDER_OPENAI_SEARCH_MODEL`: Model for web search (optional)
- `LLM_SPIDER_OPENAI_SELECT_MODEL`: Model for child-link selection (optional)
- `LLM_SPIDER_OPENAI_REASONING_EFFORT`: `none|minimal|low|medium|high|xhigh` (default: `medium`)

`reasoning.effort` is only sent for reasoning-capable models (GPT-5 and o-series).
For other models, it is ignored.

## Minimal example

```rust
use std::time::Duration;

use anyhow::Context as _;
use llm_spider::openai::{OpenAiClient, ReasoningEffort};
use llm_spider::spider::{compose_markdown, crawl, UserRequest};

fn main() -> anyhow::Result<()> {
    llm_spider::logging::init().context("init logging")?;

    let request = UserRequest {
        query: "example query".to_owned(),
        max_chars: 4000,
        min_sources: 3,
        search_limit: 10,
        max_pages: 20,
        max_depth: 1,
        max_elapsed: Duration::from_secs(30),
        max_child_candidates: 20,
        max_children_per_page: 3,
        allow_local: false,
    };

    let openai = OpenAiClient::from_env()
        .context("init OpenAI client")?
        .with_reasoning_effort(ReasoningEffort::Medium);

    let result = crawl(&request, &openai).context("crawl")?;
    let markdown = compose_markdown(&request, &result);
    println!("{markdown}");
    Ok(())
}
```

## Notes

- Logging is powered by `tracing` and controlled via `RUST_LOG`.
- Crawling respects `robots.txt` and applies host-level rate limiting.
